{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adec70bf",
   "metadata": {},
   "source": [
    "# LLM Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba16a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key=os.environ.get(\"openrouter_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e626949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요. 저는 어떻게 도와드릴까요?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 37, 'total_tokens': 50, 'completion_time': 0.047272727, 'prompt_time': 0.002050713, 'queue_time': 0.24800220399999998, 'total_time': 0.04932344}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--789cafab-08a1-492e-a3d4-2b52c9bf6f8d-0', usage_metadata={'input_tokens': 37, 'output_tokens': 13, 'total_tokens': 50})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"qwen/qwen3-14b:free\", # \"qwen/qwen3-14b:free\", \"qwen/qwen3-30b-a3b:free\",\n",
    "#     base_url=\"https://openrouter.ai/api/v1\",\n",
    "#     api_key=api_key,\n",
    "#     )\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, max_tokens=3000,) # \"gemma2-9b-it\", qwen-qwq-32b \"llama-3.3-70b-versatile\"  llama-3.1-8b-instant\n",
    "\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"qwen3:4b\",\n",
    "#     base_url=\"http://localhost:11434/v1\",\n",
    "#     api_key=\"ollama\",\n",
    "# )\n",
    "\n",
    "llm.invoke(\"안녕\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b332b",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Any, Optional, Dict \n",
    "\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from dataclasses import dataclass\n",
    "\n",
    "DEFAULT_REPORT_STRUCTURE = \"\"\"Use this structure to create a report on the user-provided topic:\n",
    "\n",
    "1. Introduction (no research needed)\n",
    "   - Brief overview of the topic area\n",
    "\n",
    "2. Main Body Sections:\n",
    "   - Each section should focus on a sub-topic of the user-provided topic\n",
    "   \n",
    "3. Conclusion\n",
    "   - Aim for 1 structural element (either a list of table) that distills the main body sections \n",
    "   - Provide a concise summary of the report\"\"\"\n",
    "\n",
    "class SearchAPI(Enum):\n",
    "    PERPLEXITY = \"perplexity\"\n",
    "    TAVILY = \"tavily\"\n",
    "    ARXIV = \"arxiv\"\n",
    "    PUBMED = \"pubmed\"\n",
    "    DUCKDUCKGO = \"duckduckgo\"\n",
    "    GOOGLESEARCH = \"googlesearch\"\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields for the chatbot.\"\"\"\n",
    "    # Common configuration\n",
    "    report_structure: str = DEFAULT_REPORT_STRUCTURE # Defaults to the default report structure\n",
    "    search_api: SearchAPI = SearchAPI.TAVILY # Default to TAVILY\n",
    "    search_api_config: Optional[Dict[str, Any]] = None\n",
    "    \n",
    "    # Graph-specific configuration\n",
    "    number_of_queries: int = 2 # Number of search queries to generate per iteration\n",
    "    max_search_depth: int = 2 # Maximum number of reflection + search iterations\n",
    "    planner_provider: str = \"anthropic\"  # Defaults to Anthropic as provider\n",
    "    planner_model: str = \"claude-3-7-sonnet-latest\" # Defaults to claude-3-7-sonnet-latest\n",
    "    planner_model_kwargs: Optional[Dict[str, Any]] = None # kwargs for planner_model\n",
    "    writer_provider: str = \"anthropic\" # Defaults to Anthropic as provider\n",
    "    writer_model: str = \"claude-3-5-sonnet-latest\" # Defaults to claude-3-5-sonnet-latest\n",
    "    writer_model_kwargs: Optional[Dict[str, Any]] = None # kwargs for writer_model\n",
    "    search_api: SearchAPI = SearchAPI.TAVILY # Default to TAVILY\n",
    "    search_api_config: Optional[Dict[str, Any]] = None \n",
    "    \n",
    "    # Multi-agent specific configuration\n",
    "    supervisor_model: str = \"openai:gpt-4.1\" # Model for supervisor agent in multi-agent setup\n",
    "    researcher_model: str = \"openai:gpt-4.1\" # Model for research agents in multi-agent setup \n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f950f56",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc341df",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_planner_query_writer_instructions=\"\"\"You are performing research for a report. \n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Report organization>\n",
    "{report_organization}\n",
    "</Report organization>\n",
    "\n",
    "<Task>\n",
    "Your goal is to generate {number_of_queries} web search queries that will help gather information for planning the report sections. \n",
    "\n",
    "The queries should:\n",
    "\n",
    "1. Be related to the Report topic\n",
    "2. Help satisfy the requirements specified in the report organization\n",
    "\n",
    "Make the queries specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.\n",
    "</Task>\n",
    "\n",
    "<Format>\n",
    "Call the Queries tool \n",
    "</Format>\n",
    "\"\"\"\n",
    "\n",
    "report_planner_instructions=\"\"\"I want a plan for a report that is concise and focused.\n",
    "\n",
    "<Report topic>\n",
    "The topic of the report is:\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Report organization>\n",
    "The report should follow this organization: \n",
    "{report_organization}\n",
    "</Report organization>\n",
    "\n",
    "<Context>\n",
    "Here is context to use to plan the sections of the report: \n",
    "{context}\n",
    "</Context>\n",
    "\n",
    "<Task>\n",
    "Generate a list of sections for the report. Your plan should be tight and focused with NO overlapping sections or unnecessary filler. \n",
    "\n",
    "For example, a good report structure might look like:\n",
    "1/ intro\n",
    "2/ overview of topic A\n",
    "3/ overview of topic B\n",
    "4/ comparison between A and B\n",
    "5/ conclusion\n",
    "\n",
    "Each section should have the fields:\n",
    "\n",
    "- Name - Name for this section of the report.\n",
    "- Description - Brief overview of the main topics covered in this section.\n",
    "- Research - Whether to perform web research for this section of the report. IMPORTANT: Main body sections (not intro/conclusion) MUST have Research=True. A report must have AT LEAST 2-3 sections with Research=True to be useful.\n",
    "- Content - The content of the section, which you will leave blank for now.\n",
    "\n",
    "Integration guidelines:\n",
    "- Include examples and implementation details within main topic sections, not as separate sections\n",
    "- Ensure each section has a distinct purpose with no content overlap\n",
    "- Combine related concepts rather than separating them\n",
    "- CRITICAL: Every section MUST be directly relevant to the main topic\n",
    "- Avoid tangential or loosely related sections that don't directly address the core topic\n",
    "\n",
    "Before submitting, review your structure to ensure it has no redundant sections and follows a logical flow.\n",
    "</Task>\n",
    "\n",
    "<Feedback>\n",
    "Here is feedback on the report structure from review (if any):\n",
    "{feedback}\n",
    "</Feedback>\n",
    "\n",
    "<Format>\n",
    "Call the Sections tool \n",
    "</Format>\n",
    "\"\"\"\n",
    "\n",
    "query_writer_instructions=\"\"\"You are an expert technical writer crafting targeted web search queries that will gather comprehensive information for writing a technical report section.\n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Section topic>\n",
    "{section_topic}\n",
    "</Section topic>\n",
    "\n",
    "<Task>\n",
    "Your goal is to generate {number_of_queries} search queries that will help gather comprehensive information above the section topic. \n",
    "\n",
    "The queries should:\n",
    "\n",
    "1. Be related to the topic \n",
    "2. Examine different aspects of the topic\n",
    "\n",
    "Make the queries specific enough to find high-quality, relevant sources.\n",
    "</Task>\n",
    "\n",
    "<Format>\n",
    "Call the Queries tool \n",
    "</Format>\n",
    "\"\"\"\n",
    "\n",
    "section_writer_instructions = \"\"\"Write one section of a research report.\n",
    "\n",
    "<Task>\n",
    "1. Review the report topic, section name, and section topic carefully.\n",
    "2. If present, review any existing section content. \n",
    "3. Then, look at the provided Source material.\n",
    "4. Decide the sources that you will use it to write a report section.\n",
    "5. Write the report section and list your sources. \n",
    "</Task>\n",
    "\n",
    "<Writing Guidelines>\n",
    "- If existing section content is not populated, write from scratch\n",
    "- If existing section content is populated, synthesize it with the source material\n",
    "- Strict 150-200 word limit\n",
    "- Use simple, clear language\n",
    "- Use short paragraphs (2-3 sentences max)\n",
    "- Use ## for section title (Markdown format)\n",
    "</Writing Guidelines>\n",
    "\n",
    "<Citation Rules>\n",
    "- Assign each unique URL a single citation number in your text\n",
    "- End with ### Sources that lists each source with corresponding numbers\n",
    "- IMPORTANT: Number sources sequentially without gaps (1,2,3,4...) in the final list regardless of which sources you choose\n",
    "- Example format:\n",
    "  [1] Source Title: URL\n",
    "  [2] Source Title: URL\n",
    "</Citation Rules>\n",
    "\n",
    "<Final Check>\n",
    "1. Verify that EVERY claim is grounded in the provided Source material\n",
    "2. Confirm each URL appears ONLY ONCE in the Source list\n",
    "3. Verify that sources are numbered sequentially (1,2,3...) without any gaps\n",
    "</Final Check>\n",
    "\"\"\"\n",
    "\n",
    "section_writer_inputs=\"\"\" \n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Section name>\n",
    "{section_name}\n",
    "</Section name>\n",
    "\n",
    "<Section topic>\n",
    "{section_topic}\n",
    "</Section topic>\n",
    "\n",
    "<Existing section content (if populated)>\n",
    "{section_content}\n",
    "</Existing section content>\n",
    "\n",
    "<Source material>\n",
    "{context}\n",
    "</Source material>\n",
    "\"\"\"\n",
    "\n",
    "section_grader_instructions = \"\"\"Review a report section relative to the specified topic:\n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<section topic>\n",
    "{section_topic}\n",
    "</section topic>\n",
    "\n",
    "<section content>\n",
    "{section}\n",
    "</section content>\n",
    "\n",
    "<task>\n",
    "Evaluate whether the section content adequately addresses the section topic.\n",
    "\n",
    "If the section content does not adequately address the section topic, generate {number_of_follow_up_queries} follow-up search queries to gather missing information.\n",
    "</task>\n",
    "\n",
    "<format>\n",
    "Call the Feedback tool and output with the following schema:\n",
    "\n",
    "grade: Literal[\"pass\",\"fail\"] = Field(\n",
    "    description=\"Evaluation result indicating whether the response meets requirements ('pass') or needs revision ('fail').\"\n",
    ")\n",
    "follow_up_queries: List[SearchQuery] = Field(\n",
    "    description=\"List of follow-up search queries.\",\n",
    ")\n",
    "</format>\n",
    "\"\"\"\n",
    "\n",
    "final_section_writer_instructions=\"\"\"You are an expert technical writer crafting a section that synthesizes information from the rest of the report.\n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Section name>\n",
    "{section_name}\n",
    "</Section name>\n",
    "\n",
    "<Section topic> \n",
    "{section_topic}\n",
    "</Section topic>\n",
    "\n",
    "<Available report content>\n",
    "{context}\n",
    "</Available report content>\n",
    "\n",
    "<Task>\n",
    "1. Section-Specific Approach:\n",
    "\n",
    "For Introduction:\n",
    "- Use # for report title (Markdown format)\n",
    "- 50-100 word limit\n",
    "- Write in simple and clear language\n",
    "- Focus on the core motivation for the report in 1-2 paragraphs\n",
    "- Use a clear narrative arc to introduce the report\n",
    "- Include NO structural elements (no lists or tables)\n",
    "- No sources section needed\n",
    "\n",
    "For Conclusion/Summary:\n",
    "- Use ## for section title (Markdown format)\n",
    "- 100-150 word limit\n",
    "- For comparative reports:\n",
    "    * Must include a focused comparison table using Markdown table syntax\n",
    "    * Table should distill insights from the report\n",
    "    * Keep table entries clear and concise\n",
    "- For non-comparative reports: \n",
    "    * Only use ONE structural element IF it helps distill the points made in the report:\n",
    "    * Either a focused table comparing items present in the report (using Markdown table syntax)\n",
    "    * Or a short list using proper Markdown list syntax:\n",
    "      - Use `*` or `-` for unordered lists\n",
    "      - Use `1.` for ordered lists\n",
    "      - Ensure proper indentation and spacing\n",
    "- End with specific next steps or implications\n",
    "- No sources section needed\n",
    "\n",
    "3. Writing Approach:\n",
    "- Use concrete details over general statements\n",
    "- Make every word count\n",
    "- Focus on your single most important point\n",
    "</Task>\n",
    "\n",
    "<Quality Checks>\n",
    "- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section\n",
    "- For conclusion: 100-150 word limit, ## for section title, only ONE structural element at most, no sources section\n",
    "- Markdown format\n",
    "- Do not include word count or any preamble in your response\n",
    "</Quality Checks>\"\"\"\n",
    "\n",
    "\n",
    "## Supervisor\n",
    "SUPERVISOR_INSTRUCTIONS = \"\"\"\n",
    "You are scoping research for a report based on a user-provided topic.\n",
    "\n",
    "### Your responsibilities:\n",
    "\n",
    "1. **Gather Background Information**  \n",
    "   Based upon the user's topic, use the `enhanced_tavily_search` to collect relevant information about the topic. \n",
    "   - You MUST perform ONLY ONE search to gather comprehensive context\n",
    "   - Create a highly targeted search query that will yield the most valuable information\n",
    "   - Take time to analyze and synthesize the search results before proceeding\n",
    "   - Do not proceed to the next step until you have an understanding of the topic\n",
    "\n",
    "2. **Clarify the Topic**  \n",
    "   After your initial research, engage with the user to clarify any questions that arose.\n",
    "   - Ask ONE SET of follow-up questions based on what you learned from your searches\n",
    "   - Do not proceed until you fully understand the topic, goals, constraints, and any preferences\n",
    "   - Synthesize what you've learned so far before asking questions\n",
    "   - You MUST engage in at least one clarification exchange with the user before proceeding\n",
    "\n",
    "3. **Define Report Structure**  \n",
    "   Only after completing both research AND clarification with the user:\n",
    "   - Use the `Sections` tool to define a list of report sections\n",
    "   - Each section should be a written description with: a section name and a section research plan\n",
    "   - Do not include sections for introductions or conclusions (We'll add these later)\n",
    "   - Ensure sections are scoped to be independently researchable\n",
    "   - Base your sections on both the search results AND user clarifications\n",
    "   - Format your sections as a list of strings, with each string having the scope of research for that section.\n",
    "\n",
    "4. **Assemble the Final Report**  \n",
    "   When all sections are returned:\n",
    "   - IMPORTANT: First check your previous messages to see what you've already completed\n",
    "   - If you haven't created an introduction yet, use the `Introduction` tool to generate one\n",
    "     - Set content to include report title with a single # (H1 level) at the beginning\n",
    "     - Example: \"# [Report Title]\\n\\n[Introduction content...]\"\n",
    "   - After the introduction, use the `Conclusion` tool to summarize key insights\n",
    "     - Set content to include conclusion title with ## (H2 level) at the beginning\n",
    "     - Example: \"## Conclusion\\n\\n[Conclusion content...]\"\n",
    "     - Only use ONE structural element IF it helps distill the points made in the report:\n",
    "     - Either a focused table comparing items present in the report (using Markdown table syntax)\n",
    "     - Or a short list using proper Markdown list syntax:\n",
    "      - Use `*` or `-` for unordered lists\n",
    "      - Use `1.` for ordered lists\n",
    "      - Ensure proper indentation and spacing\n",
    "   - Do not call the same tool twice - check your message history\n",
    "\n",
    "### Additional Notes:\n",
    "- You are a reasoning model. Think through problems step-by-step before acting.\n",
    "- IMPORTANT: Do not rush to create the report structure. Gather information thoroughly first.\n",
    "- Use multiple searches to build a complete picture before drawing conclusions.\n",
    "- Maintain a clear, informative, and professional tone throughout.\"\"\"\n",
    "\n",
    "RESEARCH_INSTRUCTIONS = \"\"\"\n",
    "You are a researcher responsible for completing a specific section of a report.\n",
    "\n",
    "### Your goals:\n",
    "\n",
    "1. **Understand the Section Scope**  \n",
    "   Begin by reviewing the section scope of work. This defines your research focus. Use it as your objective.\n",
    "\n",
    "<Section Description>\n",
    "{section_description}\n",
    "</Section Description>\n",
    "\n",
    "2. **Strategic Research Process**  \n",
    "   Follow this precise research strategy:\n",
    "\n",
    "   a) **First Query**: Begin with a SINGLE, well-crafted search query with `enhanced_tavily_search` that directly addresses the core of the section topic.\n",
    "      - Formulate ONE targeted query that will yield the most valuable information\n",
    "      - Avoid generating multiple similar queries (e.g., 'Benefits of X', 'Advantages of X', 'Why use X')\n",
    "      - Example: \"Model Context Protocol developer benefits and use cases\" is better than separate queries for benefits and use cases\n",
    "\n",
    "   b) **Analyze Results Thoroughly**: After receiving search results:\n",
    "      - Carefully read and analyze ALL provided content\n",
    "      - Identify specific aspects that are well-covered and those that need more information\n",
    "      - Assess how well the current information addresses the section scope\n",
    "\n",
    "   c) **Follow-up Research**: If needed, conduct targeted follow-up searches:\n",
    "      - Create ONE follow-up query that addresses SPECIFIC missing information\n",
    "      - Example: If general benefits are covered but technical details are missing, search for \"Model Context Protocol technical implementation details\"\n",
    "      - AVOID redundant queries that would return similar information\n",
    "\n",
    "   d) **Research Completion**: Continue this focused process until you have:\n",
    "      - Comprehensive information addressing ALL aspects of the section scope\n",
    "      - At least 3 high-quality sources with diverse perspectives\n",
    "      - Both breadth (covering all aspects) and depth (specific details) of information\n",
    "\n",
    "3. **Use the Section Tool**  \n",
    "   Only after thorough research, write a high-quality section using the Section tool:\n",
    "   - `name`: The title of the section\n",
    "   - `description`: The scope of research you completed (brief, 1-2 sentences)\n",
    "   - `content`: The completed body of text for the section, which MUST:\n",
    "     - Begin with the section title formatted as \"## [Section Title]\" (H2 level with ##)\n",
    "     - Be formatted in Markdown style\n",
    "     - Be MAXIMUM 200 words (strictly enforce this limit)\n",
    "     - End with a \"### Sources\" subsection (H3 level with ###) containing a numbered list of URLs used\n",
    "     - Use clear, concise language with bullet points where appropriate\n",
    "     - Include relevant facts, statistics, or expert opinions\n",
    "\n",
    "Example format for content:\n",
    "```\n",
    "## [Section Title]\n",
    "\n",
    "[Body text in markdown format, maximum 200 words...]\n",
    "\n",
    "### Sources\n",
    "1. [URL 1]\n",
    "2. [URL 2]\n",
    "3. [URL 3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Research Decision Framework\n",
    "\n",
    "Before each search query or when writing the section, think through:\n",
    "\n",
    "1. **What information do I already have?**\n",
    "   - Review all information gathered so far\n",
    "   - Identify the key insights and facts already discovered\n",
    "\n",
    "2. **What information is still missing?**\n",
    "   - Identify specific gaps in knowledge relative to the section scope\n",
    "   - Prioritize the most important missing information\n",
    "\n",
    "3. **What is the most effective next action?**\n",
    "   - Determine if another search is needed (and what specific aspect to search for)\n",
    "   - Or if enough information has been gathered to write a comprehensive section\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- Focus on QUALITY over QUANTITY of searches\n",
    "- Each search should have a clear, distinct purpose\n",
    "- Do not write introductions or conclusions unless explicitly part of your section\n",
    "- Keep a professional, factual tone\n",
    "- Always follow markdown formatting\n",
    "- Stay within the 200 word limit for the main content\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a014cb",
   "metadata": {},
   "source": [
    "# States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29065e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "    research: bool = Field(\n",
    "        description=\"Whether to perform web research for this section of the report.\"\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the section.\"\n",
    "    )   \n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query for web search.\")\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[SearchQuery] = Field(\n",
    "        description=\"List of search queries.\",\n",
    "    )\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"pass\",\"fail\"] = Field(\n",
    "        description=\"Evaluation result indicating whether the response meets requirements ('pass') or needs revision ('fail').\"\n",
    "    )\n",
    "    follow_up_queries: List[SearchQuery] = Field(\n",
    "        description=\"List of follow-up search queries.\",\n",
    "    )\n",
    "\n",
    "class ReportStateInput(TypedDict):\n",
    "    topic: str # Report topic\n",
    "    \n",
    "class ReportStateOutput(TypedDict):\n",
    "    final_report: str # Final report\n",
    "\n",
    "class ReportState(TypedDict):\n",
    "    topic: str # Report topic    \n",
    "    feedback_on_report_plan: str # Feedback on the report plan\n",
    "    sections: list[Section] # List of report sections \n",
    "    completed_sections: Annotated[list, operator.add] # Send() API key\n",
    "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
    "    final_report: str # Final report\n",
    "\n",
    "class SectionState(TypedDict):\n",
    "    topic: str # Report topic\n",
    "    section: Section # Report section  \n",
    "    search_iterations: int # Number of search iterations done\n",
    "    search_queries: list[SearchQuery] # List of search queries\n",
    "    source_str: str # String of formatted source content from web search\n",
    "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "class SectionOutputState(TypedDict):\n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82d252",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import requests\n",
    "import random \n",
    "import concurrent\n",
    "import aiohttp\n",
    "import httpx\n",
    "import time\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "from urllib.parse import unquote\n",
    "\n",
    "from tavily import AsyncTavilyClient\n",
    "from duckduckgo_search import DDGS \n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify\n",
    "\n",
    "from langchain_community.retrievers import ArxivRetriever\n",
    "from langchain_community.utilities.pubmed import PubMedAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langsmith import traceable\n",
    "    \n",
    "def get_config_value(value):\n",
    "    \"\"\"\n",
    "    Helper function to handle string, dict, and enum cases of configuration values\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value\n",
    "    elif isinstance(value, dict):\n",
    "        return value\n",
    "    else:\n",
    "        return value.value\n",
    "\n",
    "def get_search_params(search_api: str, search_api_config: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Filters the search_api_config dictionary to include only parameters accepted by the specified search API.\n",
    "\n",
    "    Args:\n",
    "        search_api (str): The search API identifier (e.g., \"exa\", \"tavily\").\n",
    "        search_api_config (Optional[Dict[str, Any]]): The configuration dictionary for the search API.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary of parameters to pass to the search function.\n",
    "    \"\"\"\n",
    "    # Define accepted parameters for each search API\n",
    "    SEARCH_API_PARAMS = {\n",
    "        \"exa\": [\"max_characters\", \"num_results\", \"include_domains\", \"exclude_domains\", \"subpages\"],\n",
    "        \"tavily\": [\"max_results\", \"topic\"],\n",
    "        \"perplexity\": [],  # Perplexity accepts no additional parameters\n",
    "        \"arxiv\": [\"load_max_docs\", \"get_full_documents\", \"load_all_available_meta\"],\n",
    "        \"pubmed\": [\"top_k_results\", \"email\", \"api_key\", \"doc_content_chars_max\"],\n",
    "        \"linkup\": [\"depth\"],\n",
    "    }\n",
    "\n",
    "    # Get the list of accepted parameters for the given search API\n",
    "    accepted_params = SEARCH_API_PARAMS.get(search_api, [])\n",
    "\n",
    "    # If no config provided, return an empty dict\n",
    "    if not search_api_config:\n",
    "        return {}\n",
    "\n",
    "    # Filter the config to only include accepted parameters\n",
    "    return {k: v for k, v in search_api_config.items() if k in accepted_params}\n",
    "\n",
    "def deduplicate_and_format_sources(search_response, max_tokens_per_source=5000, include_raw_content=True):\n",
    "    \"\"\"\n",
    "    Takes a list of search responses and formats them into a readable string.\n",
    "    Limits the raw_content to approximately max_tokens_per_source tokens.\n",
    " \n",
    "    Args:\n",
    "        search_responses: List of search response dicts, each containing:\n",
    "            - query: str\n",
    "            - results: List of dicts with fields:\n",
    "                - title: str\n",
    "                - url: str\n",
    "                - content: str\n",
    "                - score: float\n",
    "                - raw_content: str|None\n",
    "        max_tokens_per_source: int\n",
    "        include_raw_content: bool\n",
    "            \n",
    "    Returns:\n",
    "        str: Formatted string with deduplicated sources\n",
    "    \"\"\"\n",
    "     # Collect all results\n",
    "    sources_list = []\n",
    "    for response in search_response:\n",
    "        sources_list.extend(response['results'])\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    unique_sources = {source['url']: source for source in sources_list}\n",
    "\n",
    "    # Format output\n",
    "    formatted_text = \"Content from sources:\\n\"\n",
    "    for i, source in enumerate(unique_sources.values(), 1):\n",
    "        formatted_text += f\"{'='*80}\\n\"  # Clear section separator\n",
    "        formatted_text += f\"Source: {source['title']}\\n\"\n",
    "        formatted_text += f\"{'-'*80}\\n\"  # Subsection separator\n",
    "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
    "        if include_raw_content:\n",
    "            # Using rough estimate of 4 characters per token\n",
    "            char_limit = max_tokens_per_source * 4\n",
    "            # Handle None raw_content\n",
    "            raw_content = source.get('raw_content', '')\n",
    "            if raw_content is None:\n",
    "                raw_content = ''\n",
    "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
    "        formatted_text += f\"{'='*80}\\n\\n\" # End section separator\n",
    "                \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def format_sections(sections: list[Section]) -> str:\n",
    "    \"\"\" Format a list of sections into a string \"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for idx, section in enumerate(sections, 1):\n",
    "        formatted_str += f\"\"\"\n",
    "{'='*60}\n",
    "Section {idx}: {section.name}\n",
    "{'='*60}\n",
    "Description:\n",
    "{section.description}\n",
    "Requires Research: \n",
    "{section.research}\n",
    "\n",
    "Content:\n",
    "{section.content if section.content else '[Not yet written]'}\n",
    "\n",
    "\"\"\"\n",
    "    return formatted_str\n",
    "\n",
    "@traceable\n",
    "async def tavily_search_async(search_queries, max_results: int = 5, topic: str = \"general\", include_raw_content: bool = True):\n",
    "    \"\"\"\n",
    "    Performs concurrent web searches with the Tavily API\n",
    "\n",
    "    Args:\n",
    "        search_queries (List[str]): List of search queries to process\n",
    "\n",
    "    Returns:\n",
    "            List[dict]: List of search responses from Tavily API:\n",
    "                {\n",
    "                    'query': str,\n",
    "                    'follow_up_questions': None,      \n",
    "                    'answer': None,\n",
    "                    'images': list,\n",
    "                    'results': [                     # List of search results\n",
    "                        {\n",
    "                            'title': str,            # Title of the webpage\n",
    "                            'url': str,              # URL of the result\n",
    "                            'content': str,          # Summary/snippet of content\n",
    "                            'score': float,          # Relevance score\n",
    "                            'raw_content': str|None  # Full page content if available\n",
    "                        },\n",
    "                        ...\n",
    "                    ]\n",
    "                }\n",
    "    \"\"\"\n",
    "    tavily_async_client = AsyncTavilyClient()\n",
    "    search_tasks = []\n",
    "    for query in search_queries:\n",
    "            search_tasks.append(\n",
    "                tavily_async_client.search(\n",
    "                    query,\n",
    "                    max_results=max_results,\n",
    "                    include_raw_content=include_raw_content,\n",
    "                    topic=topic\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Execute all searches concurrently\n",
    "    search_docs = await asyncio.gather(*search_tasks)\n",
    "    return search_docs\n",
    "\n",
    "@traceable\n",
    "def perplexity_search(search_queries):\n",
    "    \"\"\"Search the web using the Perplexity API.\n",
    "    \n",
    "    Args:\n",
    "        search_queries (List[SearchQuery]): List of search queries to process\n",
    "  \n",
    "    Returns:\n",
    "        List[dict]: List of search responses from Perplexity API, one per query. Each response has format:\n",
    "            {\n",
    "                'query': str,                    # The original search query\n",
    "                'follow_up_questions': None,      \n",
    "                'answer': None,\n",
    "                'images': list,\n",
    "                'results': [                     # List of search results\n",
    "                    {\n",
    "                        'title': str,            # Title of the search result\n",
    "                        'url': str,              # URL of the result\n",
    "                        'content': str,          # Summary/snippet of content\n",
    "                        'score': float,          # Relevance score\n",
    "                        'raw_content': str|None  # Full content or None for secondary citations\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.getenv('PERPLEXITY_API_KEY')}\"\n",
    "    }\n",
    "    \n",
    "    search_docs = []\n",
    "    for query in search_queries:\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"sonar-pro\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Search the web and provide factual information with sources.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": query\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"https://api.perplexity.ai/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        # Parse the response\n",
    "        data = response.json()\n",
    "        content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        citations = data.get(\"citations\", [\"https://perplexity.ai\"])\n",
    "        \n",
    "        # Create results list for this query\n",
    "        results = []\n",
    "        \n",
    "        # First citation gets the full content\n",
    "        results.append({\n",
    "            \"title\": f\"Perplexity Search, Source 1\",\n",
    "            \"url\": citations[0],\n",
    "            \"content\": content,\n",
    "            \"raw_content\": content,\n",
    "            \"score\": 1.0  # Adding score to match Tavily format\n",
    "        })\n",
    "        \n",
    "        # Add additional citations without duplicating content\n",
    "        for i, citation in enumerate(citations[1:], start=2):\n",
    "            results.append({\n",
    "                \"title\": f\"Perplexity Search, Source {i}\",\n",
    "                \"url\": citation,\n",
    "                \"content\": \"See primary source for full content\",\n",
    "                \"raw_content\": None,\n",
    "                \"score\": 0.5  # Lower score for secondary sources\n",
    "            })\n",
    "        \n",
    "        # Format response to match Tavily structure\n",
    "        search_docs.append({\n",
    "            \"query\": query,\n",
    "            \"follow_up_questions\": None,\n",
    "            \"answer\": None,\n",
    "            \"images\": [],\n",
    "            \"results\": results\n",
    "        })\n",
    "    \n",
    "    return search_docs\n",
    "\n",
    "@traceable\n",
    "async def arxiv_search_async(search_queries, load_max_docs=5, get_full_documents=True, load_all_available_meta=True):\n",
    "    \"\"\"\n",
    "    Performs concurrent searches on arXiv using the ArxivRetriever.\n",
    "\n",
    "    Args:\n",
    "        search_queries (List[str]): List of search queries or article IDs\n",
    "        load_max_docs (int, optional): Maximum number of documents to return per query. Default is 5.\n",
    "        get_full_documents (bool, optional): Whether to fetch full text of documents. Default is True.\n",
    "        load_all_available_meta (bool, optional): Whether to load all available metadata. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of search responses from arXiv, one per query. Each response has format:\n",
    "            {\n",
    "                'query': str,                    # The original search query\n",
    "                'follow_up_questions': None,      \n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [                     # List of search results\n",
    "                    {\n",
    "                        'title': str,            # Title of the paper\n",
    "                        'url': str,              # URL (Entry ID) of the paper\n",
    "                        'content': str,          # Formatted summary with metadata\n",
    "                        'score': float,          # Relevance score (approximated)\n",
    "                        'raw_content': str|None  # Full paper content if available\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    async def process_single_query(query):\n",
    "        try:\n",
    "            # Create retriever for each query\n",
    "            retriever = ArxivRetriever(\n",
    "                load_max_docs=load_max_docs,\n",
    "                get_full_documents=get_full_documents,\n",
    "                load_all_available_meta=load_all_available_meta\n",
    "            )\n",
    "            \n",
    "            # Run the synchronous retriever in a thread pool\n",
    "            loop = asyncio.get_event_loop()\n",
    "            docs = await loop.run_in_executor(None, lambda: retriever.invoke(query))\n",
    "            \n",
    "            results = []\n",
    "            # Assign decreasing scores based on the order\n",
    "            base_score = 1.0\n",
    "            score_decrement = 1.0 / (len(docs) + 1) if docs else 0\n",
    "            \n",
    "            for i, doc in enumerate(docs):\n",
    "                # Extract metadata\n",
    "                metadata = doc.metadata\n",
    "                \n",
    "                # Use entry_id as the URL (this is the actual arxiv link)\n",
    "                url = metadata.get('entry_id', '')\n",
    "                \n",
    "                # Format content with all useful metadata\n",
    "                content_parts = []\n",
    "\n",
    "                # Primary information\n",
    "                if 'Summary' in metadata:\n",
    "                    content_parts.append(f\"Summary: {metadata['Summary']}\")\n",
    "\n",
    "                if 'Authors' in metadata:\n",
    "                    content_parts.append(f\"Authors: {metadata['Authors']}\")\n",
    "\n",
    "                # Add publication information\n",
    "                published = metadata.get('Published')\n",
    "                published_str = published.isoformat() if hasattr(published, 'isoformat') else str(published) if published else ''\n",
    "                if published_str:\n",
    "                    content_parts.append(f\"Published: {published_str}\")\n",
    "\n",
    "                # Add additional metadata if available\n",
    "                if 'primary_category' in metadata:\n",
    "                    content_parts.append(f\"Primary Category: {metadata['primary_category']}\")\n",
    "\n",
    "                if 'categories' in metadata and metadata['categories']:\n",
    "                    content_parts.append(f\"Categories: {', '.join(metadata['categories'])}\")\n",
    "\n",
    "                if 'comment' in metadata and metadata['comment']:\n",
    "                    content_parts.append(f\"Comment: {metadata['comment']}\")\n",
    "\n",
    "                if 'journal_ref' in metadata and metadata['journal_ref']:\n",
    "                    content_parts.append(f\"Journal Reference: {metadata['journal_ref']}\")\n",
    "\n",
    "                if 'doi' in metadata and metadata['doi']:\n",
    "                    content_parts.append(f\"DOI: {metadata['doi']}\")\n",
    "\n",
    "                # Get PDF link if available in the links\n",
    "                pdf_link = \"\"\n",
    "                if 'links' in metadata and metadata['links']:\n",
    "                    for link in metadata['links']:\n",
    "                        if 'pdf' in link:\n",
    "                            pdf_link = link\n",
    "                            content_parts.append(f\"PDF: {pdf_link}\")\n",
    "                            break\n",
    "\n",
    "                # Join all content parts with newlines \n",
    "                content = \"\\n\".join(content_parts)\n",
    "                \n",
    "                result = {\n",
    "                    'title': metadata.get('Title', ''),\n",
    "                    'url': url,  # Using entry_id as the URL\n",
    "                    'content': content,\n",
    "                    'score': base_score - (i * score_decrement),\n",
    "                    'raw_content': doc.page_content if get_full_documents else None\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': results\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Handle exceptions gracefully\n",
    "            print(f\"Error processing arXiv query '{query}': {str(e)}\")\n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # Process queries sequentially with delay to respect arXiv rate limit (1 request per 3 seconds)\n",
    "    search_docs = []\n",
    "    for i, query in enumerate(search_queries):\n",
    "        try:\n",
    "            # Add delay between requests (3 seconds per ArXiv's rate limit)\n",
    "            if i > 0:  # Don't delay the first request\n",
    "                await asyncio.sleep(3.0)\n",
    "            \n",
    "            result = await process_single_query(query)\n",
    "            search_docs.append(result)\n",
    "        except Exception as e:\n",
    "            # Handle exceptions gracefully\n",
    "            print(f\"Error processing arXiv query '{query}': {str(e)}\")\n",
    "            search_docs.append({\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "            # Add additional delay if we hit a rate limit error\n",
    "            if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                print(\"ArXiv rate limit exceeded. Adding additional delay...\")\n",
    "                await asyncio.sleep(5.0)  # Add a longer delay if we hit a rate limit\n",
    "    \n",
    "    return search_docs\n",
    "\n",
    "@traceable\n",
    "async def pubmed_search_async(search_queries, top_k_results=5, email=None, api_key=None, doc_content_chars_max=4000):\n",
    "    \"\"\"\n",
    "    Performs concurrent searches on PubMed using the PubMedAPIWrapper.\n",
    "\n",
    "    Args:\n",
    "        search_queries (List[str]): List of search queries\n",
    "        top_k_results (int, optional): Maximum number of documents to return per query. Default is 5.\n",
    "        email (str, optional): Email address for PubMed API. Required by NCBI.\n",
    "        api_key (str, optional): API key for PubMed API for higher rate limits.\n",
    "        doc_content_chars_max (int, optional): Maximum characters for document content. Default is 4000.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of search responses from PubMed, one per query. Each response has format:\n",
    "            {\n",
    "                'query': str,                    # The original search query\n",
    "                'follow_up_questions': None,      \n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [                     # List of search results\n",
    "                    {\n",
    "                        'title': str,            # Title of the paper\n",
    "                        'url': str,              # URL to the paper on PubMed\n",
    "                        'content': str,          # Formatted summary with metadata\n",
    "                        'score': float,          # Relevance score (approximated)\n",
    "                        'raw_content': str       # Full abstract content\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    async def process_single_query(query):\n",
    "        try:\n",
    "            # print(f\"Processing PubMed query: '{query}'\")\n",
    "            \n",
    "            # Create PubMed wrapper for the query\n",
    "            wrapper = PubMedAPIWrapper(\n",
    "                top_k_results=top_k_results,\n",
    "                doc_content_chars_max=doc_content_chars_max,\n",
    "                email=email if email else \"your_email@example.com\",\n",
    "                api_key=api_key if api_key else \"\"\n",
    "            )\n",
    "            \n",
    "            # Run the synchronous wrapper in a thread pool\n",
    "            loop = asyncio.get_event_loop()\n",
    "            \n",
    "            # Use wrapper.lazy_load instead of load to get better visibility\n",
    "            docs = await loop.run_in_executor(None, lambda: list(wrapper.lazy_load(query)))\n",
    "            \n",
    "            print(f\"Query '{query}' returned {len(docs)} results\")\n",
    "            \n",
    "            results = []\n",
    "            # Assign decreasing scores based on the order\n",
    "            base_score = 1.0\n",
    "            score_decrement = 1.0 / (len(docs) + 1) if docs else 0\n",
    "            \n",
    "            for i, doc in enumerate(docs):\n",
    "                # Format content with metadata\n",
    "                content_parts = []\n",
    "                \n",
    "                if doc.get('Published'):\n",
    "                    content_parts.append(f\"Published: {doc['Published']}\")\n",
    "                \n",
    "                if doc.get('Copyright Information'):\n",
    "                    content_parts.append(f\"Copyright Information: {doc['Copyright Information']}\")\n",
    "                \n",
    "                if doc.get('Summary'):\n",
    "                    content_parts.append(f\"Summary: {doc['Summary']}\")\n",
    "                \n",
    "                # Generate PubMed URL from the article UID\n",
    "                uid = doc.get('uid', '')\n",
    "                url = f\"https://pubmed.ncbi.nlm.nih.gov/{uid}/\" if uid else \"\"\n",
    "                \n",
    "                # Join all content parts with newlines\n",
    "                content = \"\\n\".join(content_parts)\n",
    "                \n",
    "                result = {\n",
    "                    'title': doc.get('Title', ''),\n",
    "                    'url': url,\n",
    "                    'content': content,\n",
    "                    'score': base_score - (i * score_decrement),\n",
    "                    'raw_content': doc.get('Summary', '')\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': results\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Handle exceptions with more detailed information\n",
    "            error_msg = f\"Error processing PubMed query '{query}': {str(e)}\"\n",
    "            print(error_msg)\n",
    "            import traceback\n",
    "            print(traceback.format_exc())  # Print full traceback for debugging\n",
    "            \n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # Process all queries with a reasonable delay between them\n",
    "    search_docs = []\n",
    "    \n",
    "    # Start with a small delay that increases if we encounter rate limiting\n",
    "    delay = 1.0  # Start with a more conservative delay\n",
    "    \n",
    "    for i, query in enumerate(search_queries):\n",
    "        try:\n",
    "            # Add delay between requests\n",
    "            if i > 0:  # Don't delay the first request\n",
    "                # print(f\"Waiting {delay} seconds before next query...\")\n",
    "                await asyncio.sleep(delay)\n",
    "            \n",
    "            result = await process_single_query(query)\n",
    "            search_docs.append(result)\n",
    "            \n",
    "            # If query was successful with results, we can slightly reduce delay (but not below minimum)\n",
    "            if result.get('results') and len(result['results']) > 0:\n",
    "                delay = max(0.5, delay * 0.9)  # Don't go below 0.5 seconds\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle exceptions gracefully\n",
    "            error_msg = f\"Error in main loop processing PubMed query '{query}': {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            search_docs.append({\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "            # If we hit an exception, increase delay for next query\n",
    "            delay = min(5.0, delay * 1.5)  # Don't exceed 5 seconds\n",
    "    \n",
    "    return search_docs\n",
    "\n",
    "@traceable\n",
    "async def google_search_async(search_queries: Union[str, List[str]], max_results: int = 5, include_raw_content: bool = True):\n",
    "    \"\"\"\n",
    "    Performs concurrent web searches using Google.\n",
    "    Uses Google Custom Search API if environment variables are set, otherwise falls back to web scraping.\n",
    "\n",
    "    Args:\n",
    "        search_queries (List[str]): List of search queries to process\n",
    "        max_results (int): Maximum number of results to return per query\n",
    "        include_raw_content (bool): Whether to fetch full page content\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of search responses from Google, one per query\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Check for API credentials from environment variables\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    cx = os.environ.get(\"GOOGLE_CX\")\n",
    "    use_api = bool(api_key and cx)\n",
    "    \n",
    "    # Handle case where search_queries is a single string\n",
    "    if isinstance(search_queries, str):\n",
    "        search_queries = [search_queries]\n",
    "    \n",
    "    # Define user agent generator\n",
    "    def get_useragent():\n",
    "        \"\"\"Generates a random user agent string.\"\"\"\n",
    "        lynx_version = f\"Lynx/{random.randint(2, 3)}.{random.randint(8, 9)}.{random.randint(0, 2)}\"\n",
    "        libwww_version = f\"libwww-FM/{random.randint(2, 3)}.{random.randint(13, 15)}\"\n",
    "        ssl_mm_version = f\"SSL-MM/{random.randint(1, 2)}.{random.randint(3, 5)}\"\n",
    "        openssl_version = f\"OpenSSL/{random.randint(1, 3)}.{random.randint(0, 4)}.{random.randint(0, 9)}\"\n",
    "        return f\"{lynx_version} {libwww_version} {ssl_mm_version} {openssl_version}\"\n",
    "    \n",
    "    # Create executor for running synchronous operations\n",
    "    executor = None if use_api else concurrent.futures.ThreadPoolExecutor(max_workers=5)\n",
    "    \n",
    "    # Use a semaphore to limit concurrent requests\n",
    "    semaphore = asyncio.Semaphore(5 if use_api else 2)\n",
    "    \n",
    "    async def search_single_query(query):\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                results = []\n",
    "                \n",
    "                # API-based search\n",
    "                if use_api:\n",
    "                    # The API returns up to 10 results per request\n",
    "                    for start_index in range(1, max_results + 1, 10):\n",
    "                        # Calculate how many results to request in this batch\n",
    "                        num = min(10, max_results - (start_index - 1))\n",
    "                        \n",
    "                        # Make request to Google Custom Search API\n",
    "                        params = {\n",
    "                            'q': query,\n",
    "                            'key': api_key,\n",
    "                            'cx': cx,\n",
    "                            'start': start_index,\n",
    "                            'num': num\n",
    "                        }\n",
    "                        print(f\"Requesting {num} results for '{query}' from Google API...\")\n",
    "\n",
    "                        async with aiohttp.ClientSession() as session:\n",
    "                            async with session.get('https://www.googleapis.com/customsearch/v1', params=params) as response:\n",
    "                                if response.status != 200:\n",
    "                                    error_text = await response.text()\n",
    "                                    print(f\"API error: {response.status}, {error_text}\")\n",
    "                                    break\n",
    "                                    \n",
    "                                data = await response.json()\n",
    "                                \n",
    "                                # Process search results\n",
    "                                for item in data.get('items', []):\n",
    "                                    result = {\n",
    "                                        \"title\": item.get('title', ''),\n",
    "                                        \"url\": item.get('link', ''),\n",
    "                                        \"content\": item.get('snippet', ''),\n",
    "                                        \"score\": None,\n",
    "                                        \"raw_content\": item.get('snippet', '')\n",
    "                                    }\n",
    "                                    results.append(result)\n",
    "                        \n",
    "                        # Respect API quota with a small delay\n",
    "                        await asyncio.sleep(0.2)\n",
    "                        \n",
    "                        # If we didn't get a full page of results, no need to request more\n",
    "                        if not data.get('items') or len(data.get('items', [])) < num:\n",
    "                            break\n",
    "                \n",
    "                # Web scraping based search\n",
    "                else:\n",
    "                    # Add delay between requests\n",
    "                    await asyncio.sleep(0.5 + random.random() * 1.5)\n",
    "                    print(f\"Scraping Google for '{query}'...\")\n",
    "\n",
    "                    # Define scraping function\n",
    "                    def google_search(query, max_results):\n",
    "                        try:\n",
    "                            lang = \"en\"\n",
    "                            safe = \"active\"\n",
    "                            start = 0\n",
    "                            fetched_results = 0\n",
    "                            fetched_links = set()\n",
    "                            search_results = []\n",
    "                            \n",
    "                            while fetched_results < max_results:\n",
    "                                # Send request to Google\n",
    "                                resp = requests.get(\n",
    "                                    url=\"https://www.google.com/search\",\n",
    "                                    headers={\n",
    "                                        \"User-Agent\": get_useragent(),\n",
    "                                        \"Accept\": \"*/*\"\n",
    "                                    },\n",
    "                                    params={\n",
    "                                        \"q\": query,\n",
    "                                        \"num\": max_results + 2,\n",
    "                                        \"hl\": lang,\n",
    "                                        \"start\": start,\n",
    "                                        \"safe\": safe,\n",
    "                                    },\n",
    "                                    cookies = {\n",
    "                                        'CONSENT': 'PENDING+987',  # Bypasses the consent page\n",
    "                                        'SOCS': 'CAESHAgBEhIaAB',\n",
    "                                    }\n",
    "                                )\n",
    "                                resp.raise_for_status()\n",
    "                                \n",
    "                                # Parse results\n",
    "                                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                                result_block = soup.find_all(\"div\", class_=\"ezO2md\")\n",
    "                                new_results = 0\n",
    "                                \n",
    "                                for result in result_block:\n",
    "                                    link_tag = result.find(\"a\", href=True)\n",
    "                                    title_tag = link_tag.find(\"span\", class_=\"CVA68e\") if link_tag else None\n",
    "                                    description_tag = result.find(\"span\", class_=\"FrIlee\")\n",
    "                                    \n",
    "                                    if link_tag and title_tag and description_tag:\n",
    "                                        link = unquote(link_tag[\"href\"].split(\"&\")[0].replace(\"/url?q=\", \"\"))\n",
    "                                        \n",
    "                                        if link in fetched_links:\n",
    "                                            continue\n",
    "                                        \n",
    "                                        fetched_links.add(link)\n",
    "                                        title = title_tag.text\n",
    "                                        description = description_tag.text\n",
    "                                        \n",
    "                                        # Store result in the same format as the API results\n",
    "                                        search_results.append({\n",
    "                                            \"title\": title,\n",
    "                                            \"url\": link,\n",
    "                                            \"content\": description,\n",
    "                                            \"score\": None,\n",
    "                                            \"raw_content\": description\n",
    "                                        })\n",
    "                                        \n",
    "                                        fetched_results += 1\n",
    "                                        new_results += 1\n",
    "                                        \n",
    "                                        if fetched_results >= max_results:\n",
    "                                            break\n",
    "                                \n",
    "                                if new_results == 0:\n",
    "                                    break\n",
    "                                    \n",
    "                                start += 10\n",
    "                                time.sleep(1)  # Delay between pages\n",
    "                            \n",
    "                            return search_results\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in Google search for '{query}': {str(e)}\")\n",
    "                            return []\n",
    "                    \n",
    "                    # Execute search in thread pool\n",
    "                    loop = asyncio.get_running_loop()\n",
    "                    search_results = await loop.run_in_executor(\n",
    "                        executor, \n",
    "                        lambda: google_search(query, max_results)\n",
    "                    )\n",
    "                    \n",
    "                    # Process the results\n",
    "                    results = search_results\n",
    "                \n",
    "                # If requested, fetch full page content asynchronously (for both API and web scraping)\n",
    "                if include_raw_content and results:\n",
    "                    content_semaphore = asyncio.Semaphore(3)\n",
    "                    \n",
    "                    async with aiohttp.ClientSession() as session:\n",
    "                        fetch_tasks = []\n",
    "                        \n",
    "                        async def fetch_full_content(result):\n",
    "                            async with content_semaphore:\n",
    "                                url = result['url']\n",
    "                                headers = {\n",
    "                                    'User-Agent': get_useragent(),\n",
    "                                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "                                }\n",
    "                                \n",
    "                                try:\n",
    "                                    await asyncio.sleep(0.2 + random.random() * 0.6)\n",
    "                                    async with session.get(url, headers=headers, timeout=10) as response:\n",
    "                                        if response.status == 200:\n",
    "                                            # Check content type to handle binary files\n",
    "                                            content_type = response.headers.get('Content-Type', '').lower()\n",
    "                                            \n",
    "                                            # Handle PDFs and other binary files\n",
    "                                            if 'application/pdf' in content_type or 'application/octet-stream' in content_type:\n",
    "                                                # For PDFs, indicate that content is binary and not parsed\n",
    "                                                result['raw_content'] = f\"[Binary content: {content_type}. Content extraction not supported for this file type.]\"\n",
    "                                            else:\n",
    "                                                try:\n",
    "                                                    # Try to decode as UTF-8 with replacements for non-UTF8 characters\n",
    "                                                    html = await response.text(errors='replace')\n",
    "                                                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                                                    result['raw_content'] = soup.get_text()\n",
    "                                                except UnicodeDecodeError as ude:\n",
    "                                                    # Fallback if we still have decoding issues\n",
    "                                                    result['raw_content'] = f\"[Could not decode content: {str(ude)}]\"\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Warning: Failed to fetch content for {url}: {str(e)}\")\n",
    "                                    result['raw_content'] = f\"[Error fetching content: {str(e)}]\"\n",
    "                                return result\n",
    "                        \n",
    "                        for result in results:\n",
    "                            fetch_tasks.append(fetch_full_content(result))\n",
    "                        \n",
    "                        updated_results = await asyncio.gather(*fetch_tasks)\n",
    "                        results = updated_results\n",
    "                        print(f\"Fetched full content for {len(results)} results\")\n",
    "                \n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"follow_up_questions\": None,\n",
    "                    \"answer\": None,\n",
    "                    \"images\": [],\n",
    "                    \"results\": results\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Google search for query '{query}': {str(e)}\")\n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"follow_up_questions\": None,\n",
    "                    \"answer\": None,\n",
    "                    \"images\": [],\n",
    "                    \"results\": []\n",
    "                }\n",
    "    \n",
    "    try:\n",
    "        # Create tasks for all search queries\n",
    "        search_tasks = [search_single_query(query) for query in search_queries]\n",
    "        \n",
    "        # Execute all searches concurrently\n",
    "        search_results = await asyncio.gather(*search_tasks)\n",
    "        \n",
    "        return search_results\n",
    "    finally:\n",
    "        # Only shut down executor if it was created\n",
    "        if executor:\n",
    "            executor.shutdown(wait=False)\n",
    "\n",
    "async def scrape_pages(titles: List[str], urls: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Scrapes content from a list of URLs and formats it into a readable markdown document.\n",
    "    \n",
    "    This function:\n",
    "    1. Takes a list of page titles and URLs\n",
    "    2. Makes asynchronous HTTP requests to each URL\n",
    "    3. Converts HTML content to markdown\n",
    "    4. Formats all content with clear source attribution\n",
    "    \n",
    "    Args:\n",
    "        titles (List[str]): A list of page titles corresponding to each URL\n",
    "        urls (List[str]): A list of URLs to scrape content from\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted string containing the full content of each page in markdown format,\n",
    "             with clear section dividers and source attribution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an async HTTP client\n",
    "    async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:\n",
    "        pages = []\n",
    "        \n",
    "        # Fetch each URL and convert to markdown\n",
    "        for url in urls:\n",
    "            try:\n",
    "                # Fetch the content\n",
    "                response = await client.get(url)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Convert HTML to markdown if successful\n",
    "                if response.status_code == 200:\n",
    "                    # Handle different content types\n",
    "                    content_type = response.headers.get('Content-Type', '')\n",
    "                    if 'text/html' in content_type:\n",
    "                        # Convert HTML to markdown\n",
    "                        markdown_content = markdownify(response.text)\n",
    "                        pages.append(markdown_content)\n",
    "                    else:\n",
    "                        # For non-HTML content, just mention the content type\n",
    "                        pages.append(f\"Content type: {content_type} (not converted to markdown)\")\n",
    "                else:\n",
    "                    pages.append(f\"Error: Received status code {response.status_code}\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                # Handle any exceptions during fetch\n",
    "                pages.append(f\"Error fetching URL: {str(e)}\")\n",
    "        \n",
    "        # Create formatted output \n",
    "        formatted_output = f\"Search results: \\n\\n\"\n",
    "        \n",
    "        for i, (title, url, page) in enumerate(zip(titles, urls, pages)):\n",
    "            formatted_output += f\"\\n\\n--- SOURCE {i+1}: {title} ---\\n\"\n",
    "            formatted_output += f\"URL: {url}\\n\\n\"\n",
    "            formatted_output += f\"FULL CONTENT:\\n {page}\"\n",
    "            formatted_output += \"\\n\\n\" + \"-\" * 80 + \"\\n\"\n",
    "        \n",
    "    return  formatted_output\n",
    "\n",
    "@tool\n",
    "async def duckduckgo_search(search_queries: List[str]):\n",
    "    \"\"\"Perform searches using DuckDuckGo with retry logic to handle rate limits\n",
    "    \n",
    "    Args:\n",
    "        search_queries (List[str]): List of search queries to process\n",
    "        \n",
    "    Returns:\n",
    "        List[dict]: List of search results\n",
    "    \"\"\"\n",
    "    \n",
    "    async def process_single_query(query):\n",
    "        # Execute synchronous search in the event loop's thread pool\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def perform_search():\n",
    "            max_retries = 3\n",
    "            retry_count = 0\n",
    "            backoff_factor = 2.0\n",
    "            last_exception = None\n",
    "            \n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    results = []\n",
    "                    with DDGS() as ddgs:\n",
    "                        # Change query slightly and add delay between retries\n",
    "                        if retry_count > 0:\n",
    "                            # Random delay with exponential backoff\n",
    "                            delay = backoff_factor ** retry_count + random.random()\n",
    "                            print(f\"Retry {retry_count}/{max_retries} for query '{query}' after {delay:.2f}s delay\")\n",
    "                            time.sleep(delay)\n",
    "                            \n",
    "                            # Add a random element to the query to bypass caching/rate limits\n",
    "                            modifiers = ['about', 'info', 'guide', 'overview', 'details', 'explained']\n",
    "                            modified_query = f\"{query} {random.choice(modifiers)}\"\n",
    "                        else:\n",
    "                            modified_query = query\n",
    "                        \n",
    "                        # Execute search\n",
    "                        ddg_results = list(ddgs.text(modified_query, max_results=5))\n",
    "                        \n",
    "                        # Format results\n",
    "                        for i, result in enumerate(ddg_results):\n",
    "                            results.append({\n",
    "                                'title': result.get('title', ''),\n",
    "                                'url': result.get('href', ''),\n",
    "                                'content': result.get('body', ''),\n",
    "                                'score': 1.0 - (i * 0.1),  # Simple scoring mechanism\n",
    "                                'raw_content': result.get('body', '')\n",
    "                            })\n",
    "                        \n",
    "                        # Return successful results\n",
    "                        return {\n",
    "                            'query': query,\n",
    "                            'follow_up_questions': None,\n",
    "                            'answer': None,\n",
    "                            'images': [],\n",
    "                            'results': results\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    # Store the exception and retry\n",
    "                    last_exception = e\n",
    "                    retry_count += 1\n",
    "                    print(f\"DuckDuckGo search error: {str(e)}. Retrying {retry_count}/{max_retries}\")\n",
    "                    \n",
    "                    # If not a rate limit error, don't retry\n",
    "                    if \"Ratelimit\" not in str(e) and retry_count >= 1:\n",
    "                        print(f\"Non-rate limit error, stopping retries: {str(e)}\")\n",
    "                        break\n",
    "            \n",
    "            # If we reach here, all retries failed\n",
    "            print(f\"All retries failed for query '{query}': {str(last_exception)}\")\n",
    "            # Return empty results but with query info preserved\n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(last_exception)\n",
    "            }\n",
    "            \n",
    "        return await loop.run_in_executor(None, perform_search)\n",
    "\n",
    "    # Process queries with delay between them to reduce rate limiting\n",
    "    search_docs = []\n",
    "    urls = []\n",
    "    titles = []\n",
    "    for i, query in enumerate(search_queries):\n",
    "        # Add delay between queries (except first one)\n",
    "        if i > 0:\n",
    "            delay = 2.0 + random.random() * 2.0  # Random delay 2-4 seconds\n",
    "            await asyncio.sleep(delay)\n",
    "        \n",
    "        # Process the query\n",
    "        result = await process_single_query(query)\n",
    "        search_docs.append(result)\n",
    "        \n",
    "        # Safely extract URLs and titles from results, handling empty result cases\n",
    "        if result['results'] and len(result['results']) > 0:\n",
    "            for res in result['results']:\n",
    "                if 'url' in res and 'title' in res:\n",
    "                    urls.append(res['url'])\n",
    "                    titles.append(res['title'])\n",
    "    \n",
    "    # If we got any valid URLs, scrape the pages\n",
    "    if urls:\n",
    "        return await scrape_pages(titles, urls)\n",
    "    else:\n",
    "        # Return a formatted error message if no valid URLs were found\n",
    "        return \"No valid search results found. Please try different search queries or use a different search API.\"\n",
    "\n",
    "@tool\n",
    "async def tavily_search(queries: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Fetches results from Tavily search API.\n",
    "    \n",
    "    Args:\n",
    "        queries (List[str]): List of search queries\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted string of search results\n",
    "    \"\"\"\n",
    "    # Use tavily_search_async with include_raw_content=True to get content directly\n",
    "    search_results = await tavily_search_async(\n",
    "        queries,\n",
    "        max_results=5,\n",
    "        topic=\"general\",\n",
    "        include_raw_content=True\n",
    "    )\n",
    "\n",
    "    # Format the search results directly using the raw_content already provided\n",
    "    formatted_output = f\"Search results: \\n\\n\"\n",
    "    \n",
    "    # Deduplicate results by URL\n",
    "    unique_results = {}\n",
    "    for response in search_results:\n",
    "        for result in response['results']:\n",
    "            url = result['url']\n",
    "            if url not in unique_results:\n",
    "                unique_results[url] = result\n",
    "    \n",
    "    # Format the unique results\n",
    "    for i, (url, result) in enumerate(unique_results.items()):\n",
    "        formatted_output += f\"\\n\\n--- SOURCE {i+1}: {result['title']} ---\\n\"\n",
    "        formatted_output += f\"URL: {url}\\n\\n\"\n",
    "        formatted_output += f\"SUMMARY:\\n{result['content']}\\n\\n\"\n",
    "        if result.get('raw_content'):\n",
    "            formatted_output += f\"FULL CONTENT:\\n{result['raw_content'][:30000]}\"  # Limit content size\n",
    "        formatted_output += \"\\n\\n\" + \"-\" * 80 + \"\\n\"\n",
    "    \n",
    "    if unique_results:\n",
    "        return formatted_output\n",
    "    else:\n",
    "        return \"No valid search results found. Please try different search queries or use a different search API.\"\n",
    "\n",
    "async def select_and_execute_search(search_api: str, query_list: list[str], params_to_pass: dict) -> str:\n",
    "    \"\"\"Select and execute the appropriate search API.\n",
    "    \n",
    "    Args:\n",
    "        search_api: Name of the search API to use\n",
    "        query_list: List of search queries to execute\n",
    "        params_to_pass: Parameters to pass to the search API\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string containing search results\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If an unsupported search API is specified\n",
    "    \"\"\"\n",
    "    if search_api == \"tavily\":\n",
    "        # Tavily search tool used with both workflow and agent \n",
    "        return await tavily_search.ainvoke({'queries': query_list}, **params_to_pass)\n",
    "    elif search_api == \"duckduckgo\":\n",
    "        # DuckDuckGo search tool used with both workflow and agent \n",
    "        return await duckduckgo_search.ainvoke({'search_queries': query_list})\n",
    "    elif search_api == \"perplexity\":\n",
    "        search_results = perplexity_search(query_list, **params_to_pass)\n",
    "        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)\n",
    "    elif search_api == \"exa\":\n",
    "        search_results = await exa_search(query_list, **params_to_pass)\n",
    "        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)\n",
    "    elif search_api == \"arxiv\":\n",
    "        search_results = await arxiv_search_async(query_list, **params_to_pass)\n",
    "        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)\n",
    "    elif search_api == \"pubmed\":\n",
    "        search_results = await pubmed_search_async(query_list, **params_to_pass)\n",
    "        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)\n",
    "    elif search_api == \"linkup\":\n",
    "        search_results = await linkup_search(query_list, **params_to_pass)\n",
    "        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)\n",
    "    elif search_api == \"googlesearch\":\n",
    "        search_results = await google_search_async(query_list, **params_to_pass)\n",
    "        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported search API: {search_api}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c089852",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559974ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "## Nodes -- \n",
    "\n",
    "async def generate_report_plan(state: ReportState, config: RunnableConfig):\n",
    "    \"\"\"Generate the initial report plan with sections.\n",
    "    \n",
    "    This node:\n",
    "    1. Gets configuration for the report structure and search parameters\n",
    "    2. Generates search queries to gather context for planning\n",
    "    3. Performs web searches using those queries\n",
    "    4. Uses an LLM to generate a structured plan with sections\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing the report topic\n",
    "        config: Configuration for models, search APIs, etc.\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the generated sections\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs\n",
    "    topic = state[\"topic\"]\n",
    "    feedback = state.get(\"feedback_on_report_plan\", None)\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    report_structure = configurable.report_structure\n",
    "    number_of_queries = configurable.number_of_queries\n",
    "    search_api = get_config_value(configurable.search_api)\n",
    "    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty\n",
    "    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters\n",
    "\n",
    "    # Convert JSON object to string if necessary\n",
    "    if isinstance(report_structure, dict):\n",
    "        report_structure = str(report_structure)\n",
    "\n",
    "    # Set writer model (model used for query writing)\n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n",
    "    structured_llm = writer_model.with_structured_output(Queries)\n",
    "\n",
    "    # Format system instructions\n",
    "    system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)\n",
    "\n",
    "    # Generate queries  \n",
    "    results = await structured_llm.ainvoke([SystemMessage(content=system_instructions_query),\n",
    "                                     HumanMessage(content=\"Generate search queries that will help with planning the sections of the report.\")])\n",
    "\n",
    "    # Web search\n",
    "    query_list = [query.search_query for query in results.queries]\n",
    "\n",
    "    # Search the web with parameters\n",
    "    source_str = await select_and_execute_search(search_api, query_list, params_to_pass)\n",
    "\n",
    "    # Format system instructions\n",
    "    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str, feedback=feedback)\n",
    "\n",
    "    # Set the planner\n",
    "    planner_provider = get_config_value(configurable.planner_provider)\n",
    "    planner_model = get_config_value(configurable.planner_model)\n",
    "    planner_model_kwargs = get_config_value(configurable.planner_model_kwargs or {})\n",
    "\n",
    "    # Report planner instructions\n",
    "    planner_message = \"\"\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. \n",
    "                        Each section must have: name, description, plan, research, and content fields.\"\"\"\n",
    "\n",
    "    # Run the planner\n",
    "    if planner_model == \"claude-3-7-sonnet-latest\":\n",
    "        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model\n",
    "        planner_llm = init_chat_model(model=planner_model, \n",
    "                                      model_provider=planner_provider, \n",
    "                                      max_tokens=20_000, \n",
    "                                      thinking={\"type\": \"enabled\", \"budget_tokens\": 16_000})\n",
    "\n",
    "    else:\n",
    "        # With other models, thinking tokens are not specifically allocated\n",
    "        planner_llm = init_chat_model(model=planner_model, \n",
    "                                      model_provider=planner_provider,\n",
    "                                      model_kwargs=planner_model_kwargs)\n",
    "    \n",
    "    # Generate the report sections\n",
    "    structured_llm = planner_llm.with_structured_output(Sections)\n",
    "    report_sections = await structured_llm.ainvoke([SystemMessage(content=system_instructions_sections),\n",
    "                                             HumanMessage(content=planner_message)])\n",
    "\n",
    "    # Get sections\n",
    "    sections = report_sections.sections\n",
    "\n",
    "    return {\"sections\": sections}\n",
    "\n",
    "def human_feedback(state: ReportState, config: RunnableConfig) -> Command[Literal[\"generate_report_plan\",\"build_section_with_web_research\"]]:\n",
    "    \"\"\"Get human feedback on the report plan and route to next steps.\n",
    "    \n",
    "    This node:\n",
    "    1. Formats the current report plan for human review\n",
    "    2. Gets feedback via an interrupt\n",
    "    3. Routes to either:\n",
    "       - Section writing if plan is approved\n",
    "       - Plan regeneration if feedback is provided\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state with sections to review\n",
    "        config: Configuration for the workflow\n",
    "        \n",
    "    Returns:\n",
    "        Command to either regenerate plan or start section writing\n",
    "    \"\"\"\n",
    "\n",
    "    # Get sections\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state['sections']\n",
    "    sections_str = \"\\n\\n\".join(\n",
    "        f\"Section: {section.name}\\n\"\n",
    "        f\"Description: {section.description}\\n\"\n",
    "        f\"Research needed: {'Yes' if section.research else 'No'}\\n\"\n",
    "        for section in sections\n",
    "    )\n",
    "\n",
    "    # Get feedback on the report plan from interrupt\n",
    "    interrupt_message = f\"\"\"Please provide feedback on the following report plan. \n",
    "                        \\n\\n{sections_str}\\n\n",
    "                        \\nDoes the report plan meet your needs?\\nPass 'true' to approve the report plan.\\nOr, provide feedback to regenerate the report plan:\"\"\"\n",
    "    \n",
    "    feedback = interrupt(interrupt_message)\n",
    "\n",
    "    # If the user approves the report plan, kick off section writing\n",
    "    if isinstance(feedback, bool) and feedback is True:\n",
    "        # Treat this as approve and kick off section writing\n",
    "        return Command(goto=[\n",
    "            Send(\"build_section_with_web_research\", {\"topic\": topic, \"section\": s, \"search_iterations\": 0}) \n",
    "            for s in sections \n",
    "            if s.research\n",
    "        ])\n",
    "    \n",
    "    # If the user provides feedback, regenerate the report plan \n",
    "    elif isinstance(feedback, str):\n",
    "        # Treat this as feedback\n",
    "        return Command(goto=\"generate_report_plan\", \n",
    "                       update={\"feedback_on_report_plan\": feedback})\n",
    "    else:\n",
    "        raise TypeError(f\"Interrupt value of type {type(feedback)} is not supported.\")\n",
    "    \n",
    "async def generate_queries(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\"Generate search queries for researching a specific section.\n",
    "    \n",
    "    This node uses an LLM to generate targeted search queries based on the \n",
    "    section topic and description.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state containing section details\n",
    "        config: Configuration including number of queries to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the generated search queries\n",
    "    \"\"\"\n",
    "\n",
    "    # Get state \n",
    "    topic = state[\"topic\"]\n",
    "    section = state[\"section\"]\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    number_of_queries = configurable.number_of_queries\n",
    "\n",
    "    # Generate queries \n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n",
    "    structured_llm = writer_model.with_structured_output(Queries)\n",
    "\n",
    "    # Format system instructions\n",
    "    system_instructions = query_writer_instructions.format(topic=topic, \n",
    "                                                           section_topic=section.description, \n",
    "                                                           number_of_queries=number_of_queries)\n",
    "\n",
    "    # Generate queries  \n",
    "    queries = await structured_llm.ainvoke([SystemMessage(content=system_instructions),\n",
    "                                     HumanMessage(content=\"Generate search queries on the provided topic.\")])\n",
    "\n",
    "    return {\"search_queries\": queries.queries}\n",
    "\n",
    "async def search_web(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\"Execute web searches for the section queries.\n",
    "    \n",
    "    This node:\n",
    "    1. Takes the generated queries\n",
    "    2. Executes searches using configured search API\n",
    "    3. Formats results into usable context\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with search queries\n",
    "        config: Search API configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dict with search results and updated iteration count\n",
    "    \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    search_queries = state[\"search_queries\"]\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    search_api = get_config_value(configurable.search_api)\n",
    "    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty\n",
    "    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters\n",
    "\n",
    "    # Web search\n",
    "    query_list = [query.search_query for query in search_queries]\n",
    "\n",
    "    # Search the web with parameters\n",
    "    source_str = await select_and_execute_search(search_api, query_list, params_to_pass)\n",
    "\n",
    "    return {\"source_str\": source_str, \"search_iterations\": state[\"search_iterations\"] + 1}\n",
    "\n",
    "async def write_section(state: SectionState, config: RunnableConfig) -> Command[Literal[END, \"search_web\"]]:\n",
    "    \"\"\"Write a section of the report and evaluate if more research is needed.\n",
    "    \n",
    "    This node:\n",
    "    1. Writes section content using search results\n",
    "    2. Evaluates the quality of the section\n",
    "    3. Either:\n",
    "       - Completes the section if quality passes\n",
    "       - Triggers more research if quality fails\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with search results and section info\n",
    "        config: Configuration for writing and evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Command to either complete section or do more research\n",
    "    \"\"\"\n",
    "\n",
    "    # Get state \n",
    "    topic = state[\"topic\"]\n",
    "    section = state[\"section\"]\n",
    "    source_str = state[\"source_str\"]\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # Format system instructions\n",
    "    section_writer_inputs_formatted = section_writer_inputs.format(topic=topic, \n",
    "                                                             section_name=section.name, \n",
    "                                                             section_topic=section.description, \n",
    "                                                             context=source_str, \n",
    "                                                             section_content=section.content)\n",
    "\n",
    "    # Generate section  \n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n",
    "\n",
    "    section_content = await writer_model.ainvoke([SystemMessage(content=section_writer_instructions),\n",
    "                                           HumanMessage(content=section_writer_inputs_formatted)])\n",
    "    \n",
    "    # Write content to the section object  \n",
    "    section.content = section_content.content\n",
    "\n",
    "    # Grade prompt \n",
    "    section_grader_message = (\"Grade the report and consider follow-up questions for missing information. \"\n",
    "                              \"If the grade is 'pass', return empty strings for all follow-up queries. \"\n",
    "                              \"If the grade is 'fail', provide specific search queries to gather missing information.\")\n",
    "    \n",
    "    section_grader_instructions_formatted = section_grader_instructions.format(topic=topic, \n",
    "                                                                               section_topic=section.description,\n",
    "                                                                               section=section.content, \n",
    "                                                                               number_of_follow_up_queries=configurable.number_of_queries)\n",
    "\n",
    "    # Use planner model for reflection\n",
    "    planner_provider = get_config_value(configurable.planner_provider)\n",
    "    planner_model = get_config_value(configurable.planner_model)\n",
    "    planner_model_kwargs = get_config_value(configurable.planner_model_kwargs or {})\n",
    "\n",
    "    if planner_model == \"claude-3-7-sonnet-latest\":\n",
    "        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model\n",
    "        reflection_model = init_chat_model(model=planner_model, \n",
    "                                           model_provider=planner_provider, \n",
    "                                           max_tokens=20_000, \n",
    "                                           thinking={\"type\": \"enabled\", \"budget_tokens\": 16_000}).with_structured_output(Feedback)\n",
    "    else:\n",
    "        reflection_model = init_chat_model(model=planner_model, \n",
    "                                           model_provider=planner_provider, model_kwargs=planner_model_kwargs).with_structured_output(Feedback)\n",
    "    # Generate feedback\n",
    "    feedback = await reflection_model.ainvoke([SystemMessage(content=section_grader_instructions_formatted),\n",
    "                                        HumanMessage(content=section_grader_message)])\n",
    "\n",
    "    # If the section is passing or the max search depth is reached, publish the section to completed sections \n",
    "    if feedback.grade == \"pass\" or state[\"search_iterations\"] >= configurable.max_search_depth:\n",
    "        # Publish the section to completed sections \n",
    "        return  Command(\n",
    "        update={\"completed_sections\": [section]},\n",
    "        goto=END\n",
    "    )\n",
    "\n",
    "    # Update the existing section with new content and update search queries\n",
    "    else:\n",
    "        return  Command(\n",
    "        update={\"search_queries\": feedback.follow_up_queries, \"section\": section},\n",
    "        goto=\"search_web\"\n",
    "        )\n",
    "    \n",
    "async def write_final_sections(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\"Write sections that don't require research using completed sections as context.\n",
    "    \n",
    "    This node handles sections like conclusions or summaries that build on\n",
    "    the researched sections rather than requiring direct research.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with completed sections as context\n",
    "        config: Configuration for the writing model\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the newly written section\n",
    "    \"\"\"\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # Get state \n",
    "    topic = state[\"topic\"]\n",
    "    section = state[\"section\"]\n",
    "    completed_report_sections = state[\"report_sections_from_research\"]\n",
    "    \n",
    "    # Format system instructions\n",
    "    system_instructions = final_section_writer_instructions.format(topic=topic, section_name=section.name, section_topic=section.description, context=completed_report_sections)\n",
    "\n",
    "    # Generate section  \n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n",
    "    \n",
    "    section_content = await writer_model.ainvoke([SystemMessage(content=system_instructions),\n",
    "                                           HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
    "    \n",
    "    # Write content to section \n",
    "    section.content = section_content.content\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section]}\n",
    "\n",
    "def gather_completed_sections(state: ReportState):\n",
    "    \"\"\"Format completed sections as context for writing final sections.\n",
    "    \n",
    "    This node takes all completed research sections and formats them into\n",
    "    a single context string for writing summary sections.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with completed sections\n",
    "        \n",
    "    Returns:\n",
    "        Dict with formatted sections as context\n",
    "    \"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = format_sections(completed_sections)\n",
    "\n",
    "    return {\"report_sections_from_research\": completed_report_sections}\n",
    "\n",
    "def compile_final_report(state: ReportState):\n",
    "    \"\"\"Compile all sections into the final report.\n",
    "    \n",
    "    This node:\n",
    "    1. Gets all completed sections\n",
    "    2. Orders them according to original plan\n",
    "    3. Combines them into the final report\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with all completed sections\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the complete report\n",
    "    \"\"\"\n",
    "\n",
    "    # Get sections\n",
    "    sections = state[\"sections\"]\n",
    "    completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n",
    "\n",
    "    # Update sections with completed content while maintaining original order\n",
    "    for section in sections:\n",
    "        section.content = completed_sections[section.name]\n",
    "\n",
    "    # Compile final report\n",
    "    all_sections = \"\\n\\n\".join([s.content for s in sections])\n",
    "\n",
    "    return {\"final_report\": all_sections}\n",
    "\n",
    "def initiate_final_section_writing(state: ReportState):\n",
    "    \"\"\"Create parallel tasks for writing non-research sections.\n",
    "    \n",
    "    This edge function identifies sections that don't need research and\n",
    "    creates parallel writing tasks for each one.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with all sections and research context\n",
    "        \n",
    "    Returns:\n",
    "        List of Send commands for parallel section writing\n",
    "    \"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API for any sections that do not require research\n",
    "    return [\n",
    "        Send(\"write_final_sections\", {\"topic\": state[\"topic\"], \"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]}) \n",
    "        for s in state[\"sections\"] \n",
    "        if not s.research\n",
    "    ]\n",
    "\n",
    "# Report section sub-graph -- \n",
    "\n",
    "# Add nodes \n",
    "section_builder = StateGraph(SectionState, output=SectionOutputState)\n",
    "section_builder.add_node(\"generate_queries\", generate_queries)\n",
    "section_builder.add_node(\"search_web\", search_web)\n",
    "section_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# Add edges\n",
    "section_builder.add_edge(START, \"generate_queries\")\n",
    "section_builder.add_edge(\"generate_queries\", \"search_web\")\n",
    "section_builder.add_edge(\"search_web\", \"write_section\")\n",
    "\n",
    "# Outer graph for initial report plan compiling results from each section -- \n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(ReportState, input=ReportStateInput, output=ReportStateOutput, config_schema=Configuration)\n",
    "builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"build_section_with_web_research\", section_builder.compile())\n",
    "builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
    "builder.add_node(\"write_final_sections\", write_final_sections)\n",
    "builder.add_node(\"compile_final_report\", compile_final_report)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"generate_report_plan\")\n",
    "builder.add_edge(\"generate_report_plan\", \"human_feedback\")\n",
    "builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
    "builder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\n",
    "builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
    "builder.add_edge(\"compile_final_report\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45717bdf",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc016c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hrm502bcfvcas4j2ppdwhsxf` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 39815, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIStatusError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m topic = \u001b[33m\"\u001b[39m\u001b[33mOverview of Model Context Protocol (MCP), an Anthropic‑backed open standard for integrating external context and tools with LLMs. Give an architectural overview for developers, tell me about interesting MCP servers, and compare to google Agent2Agent (A2A) protocol.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run the graph workflow until first interruption (waiting for user feedback)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream({\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m:topic,}, thread, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m__interrupt__\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[32m     36\u001b[39m         interrupt_value = event[\u001b[33m'\u001b[39m\u001b[33m__interrupt__\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m].value\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2759\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2753\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2754\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   2755\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2756\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2757\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   2758\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2759\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2760\u001b[39m         loop.tasks.values(),\n\u001b[32m   2761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2762\u001b[39m         retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2763\u001b[39m         get_waiter=get_waiter,\n\u001b[32m   2764\u001b[39m     ):\n\u001b[32m   2765\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2766\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2767\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:283\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    281\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    284\u001b[39m         t,\n\u001b[32m    285\u001b[39m         retry_policy,\n\u001b[32m    286\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    287\u001b[39m         configurable={\n\u001b[32m    288\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    289\u001b[39m                 _acall,\n\u001b[32m    290\u001b[39m                 weakref.ref(t),\n\u001b[32m    291\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    292\u001b[39m                 retry=retry_policy,\n\u001b[32m    293\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    294\u001b[39m                 schedule_task=\u001b[38;5;28mself\u001b[39m.schedule_task,\n\u001b[32m    295\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    296\u001b[39m                 reraise=reraise,\n\u001b[32m    297\u001b[39m                 loop=loop,\n\u001b[32m    298\u001b[39m             ),\n\u001b[32m    299\u001b[39m         },\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    301\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:128\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, configurable)\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mgenerate_report_plan\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Generate the report sections\u001b[39;00m\n\u001b[32m    122\u001b[39m structured_llm = planner_llm.with_structured_output(Sections)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m report_sections = \u001b[38;5;28;01mawait\u001b[39;00m structured_llm.ainvoke([SystemMessage(content=system_instructions_sections),\n\u001b[32m    124\u001b[39m                                          HumanMessage(content=planner_message)])\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Get sections\u001b[39;00m\n\u001b[32m    127\u001b[39m sections = report_sections.sections\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3075\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3073\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3074\u001b[39m                 part = functools.partial(step.ainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m-> \u001b[39m\u001b[32m3075\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3076\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5429\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5422\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5423\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5424\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5427\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5428\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5429\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5430\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5431\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5432\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5433\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:392\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    384\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     **kwargs: Any,\n\u001b[32m    390\u001b[39m ) -> BaseMessage:\n\u001b[32m    391\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    393\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    394\u001b[39m         stop=stop,\n\u001b[32m    395\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    396\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    397\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    398\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    399\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    400\u001b[39m         **kwargs,\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:958\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    951\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    955\u001b[39m     **kwargs: Any,\n\u001b[32m    956\u001b[39m ) -> LLMResult:\n\u001b[32m    957\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    959\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    960\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:916\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    904\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    905\u001b[39m             *[\n\u001b[32m    906\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    914\u001b[39m             ]\n\u001b[32m    915\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    917\u001b[39m flattened_outputs = [\n\u001b[32m    918\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    920\u001b[39m ]\n\u001b[32m    921\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1084\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1082\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1085\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1086\u001b[39m     )\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1088\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:519\u001b[39m, in \u001b[36mChatGroq._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    514\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    515\u001b[39m params = {\n\u001b[32m    516\u001b[39m     **params,\n\u001b[32m    517\u001b[39m     **kwargs,\n\u001b[32m    518\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(messages=message_dicts, **params)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:722\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    543\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    544\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    588\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    589\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m    590\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    591\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    592\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    720\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    721\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    723\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    724\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    725\u001b[39m             {\n\u001b[32m    726\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    727\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    728\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mexclude_domains\u001b[39m\u001b[33m\"\u001b[39m: exclude_domains,\n\u001b[32m    729\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    730\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    731\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    732\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude_domains\u001b[39m\u001b[33m\"\u001b[39m: include_domains,\n\u001b[32m    733\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    734\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    735\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    736\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    737\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    738\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    739\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    740\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    741\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_format\u001b[39m\u001b[33m\"\u001b[39m: reasoning_format,\n\u001b[32m    742\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    743\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    744\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    745\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    746\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    747\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    748\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    749\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    750\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    751\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    752\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    753\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    754\u001b[39m             },\n\u001b[32m    755\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m    756\u001b[39m         ),\n\u001b[32m    757\u001b[39m         options=make_request_options(\n\u001b[32m    758\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    759\u001b[39m         ),\n\u001b[32m    760\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m    761\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    762\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m    763\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1710\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1697\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1698\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1705\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1706\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1707\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1708\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1709\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_PROJECT\\quick_test\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1531\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1530\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1531\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1533\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAPIStatusError\u001b[39m: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hrm502bcfvcas4j2ppdwhsxf` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 39815, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
      "During task with name 'generate_report_plan' and id '3cd4c306-810b-e480-2b6a-e552bba8aa24'"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "\n",
    "# Define report structure template and configure the research workflow\n",
    "# This sets parameters for models, search tools, and report organization\n",
    "\n",
    "REPORT_STRUCTURE = \"\"\"Use this structure to create a report on the user-provided topic:\n",
    "\n",
    "1. Introduction (no research needed)\n",
    "   - Brief overview of the topic area\n",
    "\n",
    "2. Main Body Sections:\n",
    "   - Each section should focus on a sub-topic of the user-provided topic\n",
    "   \n",
    "3. Conclusion\n",
    "   - Aim for 1 structural element (either a list of table) that distills the main body sections \n",
    "   - Provide a concise summary of the report\"\"\"\n",
    "\n",
    "\n",
    "# Configuration option 2: DeepSeek-R1-Distill-Llama-70B for planning and llama-3.3-70b-versatile for writing\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"tavily\",\n",
    "                           \"planner_provider\": \"groq\",\n",
    "                           \"planner_model\": \"llama-3.3-70b-versatile\",\n",
    "                           \"writer_provider\": \"groq\",\n",
    "                           \"writer_model\": \"llama-3.3-70b-versatile\",\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           \"max_search_depth\": 1,}\n",
    "                           }\n",
    "\n",
    "# Define research topic about Model Context Protocol\n",
    "topic = \"Overview of Model Context Protocol (MCP), an Anthropic‑backed open standard for integrating external context and tools with LLMs. Give an architectural overview for developers, tell me about interesting MCP servers, and compare to google Agent2Agent (A2A) protocol.\"\n",
    "\n",
    "# Run the graph workflow until first interruption (waiting for user feedback)\n",
    "async for event in graph.astream({\"topic\":topic,}, thread, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in event:\n",
    "        interrupt_value = event['__interrupt__'][0].value\n",
    "        display(Markdown(interrupt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1f2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab9ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
